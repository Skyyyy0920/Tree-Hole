@[TOC](目录)

`ONION: A Simple and Effective Defense Against Textual Backdoor Attacks 论文解读`


---

# 一、基本信息

1. 标题：*ONION: A Simple and Effective Defense Against Textual Backdoor Attacks*

2. 发表时间：2021

3. 出版源：EMNLP

4. 领域：NLP

5. 摘要：
   
    后门攻击是对深度神经网络(DNN)的一种紧急训练时威胁。它们可以操纵DNN的输出，具有高度的隐伏性。在自然语言处理领域，已经提出了一些攻击方法，并在多种流行的模型上获得了很高的攻击成功率。然而，针对文本后门攻击的防御研究却很少。在本文中，我们提出了一种名为ONION的简单而有效的文本后门防御方法，它基于离群词检测，据我们所知，它是第一种能够处理所有文本后门攻击情况的方法。实验证明了我们的模型对BiLSTM和BERT防御5种不同的后门攻击的有效性。

6. 主要链接：
   - Paper：https://arxiv.org/abs/2011.10369
   - Github：https://github.com/thunlp/ONION


---

# 二、研究背景

## 1. 问题定义
近年来，深度神经网络(DNN)由于其强大的性能已被部署在各种现实应用中。然而，与此同时，DNN正面临着各种各样的威胁，这引起了人们对其安全性的日益关注。后门攻击或木马攻击是对DNN的一种紧急的潜在安全威胁。

后门攻击的目的是在训练过程中向DNN模型注入后门，使受害模型
1. 在正常输入上表现正常，就像一个没有后门的良性模型一样
2. 在嵌入了预先设计的触发器的输入上产生对手指定的输出，这些触发器可以激活注入的后门。

**后门攻击非常隐蔽，一般情况下后门模型几乎无法与良性模型区分开来，除非接收嵌入的触发器输入**。因此，在现实世界中，后门攻击可能会导致严重的安全问题。例如，一个被嵌入后门的人脸识别系统投入使用，它在正常输入上的性能非常好，但它会故意将任何戴着特定眼镜的人识别为目标人。此外，越来越多的模型训练外包，包括使用第三方数据集、大型预训练模型和API，大大增加了后门攻击的风险。简而言之，后门攻击的威胁日益严重。

在计算机视觉领域已经有大量关于后门攻击的研究。最常见的攻击方法是训练数据中毒，它**通过使用一些嵌入预先设计的触发器的有毒数据来训练模型，向受害模型注入后门(我们称此过程为后门训练)**。另一方面，为了减少后门攻击，也提出了各种防御方法。

但是，在自然语言处理(NLP)领域，关于后门攻击与防御的研究尚处于起步阶段。现有的大多数研究都专注于后门攻击，并提出了一些有效的攻击方法，而对文本后门攻击的防御研究非常不足。据作者所知，只有一项专门针对文本后门防御的研究，提出了一种名为BKI的防御。BKI旨在清除可能中毒的训练样本，以麻痹后门训练，防止后门注射。因此，它只能处理预训练阶段的后门攻击，在这种情况下，对手提供了一个有毒的训练数据集，用户自行训练模型。然而，随着使用第三方预训练模型或API的流行，训练后攻击的情况更加常见，即要使用的模型可能已经被注入了后门。

本文提出了一种简单有效的文本后门防御方法，可以在两种攻击情况(即训练前攻击与训练后攻击)下工作。该方法基于测试样本检查，即从测试样本中检测并删除可能是后门触发器(或后门触发器的一部分)的单词，以**防止激活受害模型的后门**。这一想法是基于一个事实——几乎所有现有的文本后门攻击都将一段与上下文无关的文本(单词或句子)插入到原始正常样本中作为触发器。插入的内容会破坏原文的流畅性，其组成词很容易被语言模型识别为离群词。例如，Kurita等人(2020)使用单词“cf”作为后门触发器，普通的语言模型可以很容易地识别出被嵌入触发器的句子“I really love cf this 3D movie.”中的离点词。

作者称这种方法为ONION(backdOor defeNse
with outlIer wOrd detectioN，带有离群词检测的后门防御)。我们进行了大量的实验来评估ONION，使用它来保护BiLSTM和BERT免受针对三个真实数据集的几种有代表性的后门攻击。实验结果表明，在保持受害者模型在正常测试样本上的准确性的同时，ONION可以大幅降低所有后门攻击的攻击成功率(平均降低40%以上)。



## 2. 难点
1. 如何检测被嵌入的触发器
   
## 3. 相关工作
现有的关于后门攻击的研究主要在计算机视觉领域(Li et al, 2020)。各种后门攻击方法已经被提出，其中大多数是基于训练数据中毒(Chen et al, 2017;廖等，2018;刘等，2020;赵等人，2020)。同时，大量研究提出了不同的方法来保护DNN模型免受后门攻击(Liu et al, 2017,2018a;乔等，2019;Du等人，2020年)。


---

# 三、实现方法

ONION的主要目的是检测句子中的异常词，这些异常词很可能与后门触发器相关。作者认为，异常词显著地降低了句子的流畅性，删除异常词会提高句子的流畅性。句子的流畅度可以通过语言模型计算出的困惑度来衡量。

按照上述思路，作者设计了ONION的防御流程。在后门模型的推理过程中，对于包含$n$个单词的$words = w_1,...,w_n$的给定测试样本(句子)，我们首先使用语言模型计算其困惑度$p_0$。

本文选择了广泛使用的GPT-2预训练语言模型(Radford等人，2019年)，然后作者将一个词的怀疑度$f_i$定义为去除该词后句子困惑度的减量，即

$$f_i = p_0 − p_i \tag{1}$$

其中，$p_i$为不含$w_i$的句子困惑度，即$s_i = w_1,...,w_{i−1},w_{i+1},...,w_n$。

$f_i$越大，$w_i$越有可能是一个离群词。这是因为如果$w_i$是一个离群词，去掉它会大大减少句子的困惑度，相应的，$f_i = p_0−p_i$将是一个很大的正数。

**将怀疑分数$f_i > t_s$的词确定为离群词，并在将测试样本提供给后门模型之前删除它们，其中$t_s$是超参数**。

为了避免意外删除正常单词并损害模型的性能，我们可以调优一些正常样本(例如，一个验证集)上的$t_s$，使其尽可能小，同时保持模型的性能。在附录A中，作者评估了ONION的性能对ts的敏感度，如果没有可用的正态样本来调优$t_s$，我们也可以经验地将$t_s$设为0，这在后面的实验中得到了证明。

作者还基于粒子群优化(Eberhart and Kennedy, 1995)和遗传算法(Goldberg and Holland, 1988)两种组合优化算法设计了更复杂的离群词消除方法。

然而，作者发现这两种复杂方法的性能并不比ONION好，而且需要更多的处理时间。作者在附录B中给出了这两种方法的详细信息。

---

# 四、创新点

1. 填补了NLP领域对于后门攻击防御手段的空白。

2. 第一次针对训练后模型的后门攻击进行了防御



---

# 五、实验细节

## 1. 实验设置
1. 数据集：
   
   使用三个真实世界的数据集来完成不同的任务:(1)SST-2 (Socher et al, 2013)，一个二元情感分析数据集，由9612个电影评论句子组成;(2) OffensEval (Zampieri等人，2019年)，二元攻击性语言识别数据集，包含来自Twitter的14,102句句子;(3) AG News (Zhang et al, 2015)，一个四类新闻主题分类数据集，包含30,399个新闻文章句子。

2. 受害模型(Victim Models)：
   
   选取了两个比较流行的NLP模型作为受害模型:(1)BiLSTM模型，其隐藏大小为1024，字嵌入大小为300;(2) BERT，特别是BERTBASE，它有12层，768维的隐藏状态。我们在两种情况下对BERT进行后门攻击:(1)BERT- t，在后门训练后立即测试BERT，如BiLSTM;(2) BERT- f，经过后门训练后，在测试前用干净的训练数据对BERT进行微调，如Kurita等人(2020)所述。

3. 攻击模型(Attack Models)：
   
   选取了五种具有代表性的后门攻击方法:(1)BadNet (Gu et al, 2017)，随机插入一些稀有词作为触发;1 (2)BadNetm和(3)BadNeth，与BadNet类似，但使用中频和高频词作为触发，Chen et al(2020)进行了尝试;(4) ripple (Kurita et al, 2020)，它也插入稀有词作为触发器，但修改了专门针对预训练模型的后门训练过程，并调整了触发器词的嵌入。它只适用于BERT-F;(5) InSent (Dai等人，2019)，插入一个固定的句子作为后门触发器。我们按照默认的超参数和设置来实现这些攻击方法。注意(1)-(4)根据Kurita等人(2020)的句子长度，为SST-2/OffensEval/AG News插入1/3/5个不同的触发词。但(5)对所有样本只插入一句话。

4. 基准防御方法(Baseline Defense Methods)：
   
   由于唯一已知的文本后门防御方法BKI不能在训练后攻击情况下工作，因此没有现成的基线。由于后门触发器的选词具有任任性，例如任何低、中、高频词都可能是后门触发器(BadNet/BadNetm/BadNeth)，因此很难设计出基于规则或其他直接的防御方法。因此，在我们的实验中，训练后攻击情况没有基线方法。

5. 评价指标(Evaluation Metrics)：
   
   我们采用两个指标来评价后门防御方法的有效性:   
   (1)$\Delta ASR$，攻击成功率的减少量($ASR$为对嵌入触发器的测试样本的分类精度)   
   (2)$\Delta CACC$，清洁精度的减少量($CACC$为模型对正常测试样本的精度)   
   $\Delta ASR$越高，$\Delta CACC$越低越好。

## 2. 实验结果
详见原文

## 3. 对ONION的分析
基于BadNet对BERT-T在SST-2上的后门攻击结果，我们进行了一系列的定量和定性分析来解释ONION的有效性。

对于一个含有触发器的中毒测试样本，ONION平均去除0.76个触发词和0.57个正常词，对所有中毒样本的触发词检测精度和召回率分别为56.19和75.66。对于一个正常的测试样本，平均去除0.63个正常单词。一些正常的单词被错误地删除了，其中大部分是罕见的单词(这些单词和整个SST-2数据集的平均频率排名图1:SST-2上的怀疑得分分布。

分别为637,106 vs. 148,340(基于GPT-2训练语料库计算)。这是意料之中的，因为语言模型倾向于对罕见词给出较高的困惑度。但是，下面的分析将证明，错误地删除这些正常词对ASR和CACC都没有什么影响。

表2给出了去除不同触发/正常单词数量的中毒测试样本的平均ASR。

我们发现，只要保留触发词(Nt=0)，无论删除多少正常词，ASR总是100%。去除触发词可显著降低ASR(100%→18.12%)。这些结果表明，只有删除触发词才能减轻后门攻击，而删除其他词是无用的。

表3为误删不同正常单词数的正常测试样本的平均CACC。我们发现(1)大部分样本(71.2%)没有去掉正常单词;(2)去掉的正常词数似乎与CACC无关。

图1为SST-2中触发词和正常词的怀疑得分分布(fi)。我们可以看到触发词与正常触发词的区别可以通过怀疑得分来实现，这说明了ONION算法的有效性。

案例研究表4显示了一些例子，ONION删除了中毒样本和正常样本中的哪些单词。我们可以看到触发词通常有相当高的怀疑得分，总是被ONION删除，这样受害者模型的后门就不会被激活。一些正常的单词因为相对罕见的用法而被错误地删除。但是这些情况发生的概率不是很高，除去它们基本上对最终结果影响不大。

4.4与BKI相比，ONION可以在训练前和训练后的攻击情况下工作。在本节中，我们将与BKI在模型用户控制后门培训过程的预培训情况下进行比较，尽管这种情况在现实中并不常见。BERT-F在这种情况下不再可行，因为它假定攻击者操纵后门训练过程。

表5为BKI和ONION对SST-2.2不同攻击的防御结果。ONION和BKI的∆ASR平均结果为44.43% vs. 16.07%，∆CACC平均结果为1.41 vs. 0.87。ONION对模型在正常样本上的性能下降比BKI略大，但后门防御效果要好得多。这些结果表明，ONION算法在训练前攻击情况下也能很好地工作。

2 . ONION在训练前攻击情况下的防御性能与训练后攻击情况下的防御性能相同，因为ONION只处理测试样本而不干预后门训练。


---

# 六、总结

实验结果表明，对于不同的基于插入的后门攻击，甚至句子插入攻击，ONION都具有很好的防御性能。

然而，ONION也有它的局限性。一些同时进行的研究已经意识到不可见后门攻击的重要性，并提出了上下文感知的句子插入(Zhang et al, 2021)甚至非插入触发器，如句法结构(Qi et al, 2021a)和单词替换(Qi et al, 2021b)。ONION很难抵御这些秘密的后门攻击。